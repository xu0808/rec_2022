【spark】
1、计算分组count的总和
from pyspark.sql import functions as F
total = df2.agg(F.sum('count')).first()[0]

2、数值格式化
concat(format_number(100*count/{1}, 3),'%')
sql3 = "select concat({0}*setp_num + 1,'~',{0}*(setp_num+1)) as bucket,format_number(100*count/{1}, 5) as percent from data2".format(


3、组内排序(最值)
select a.Classid,a.English from
(select Classid,English,row_number() over(partition by Classid order by English desc) as n
from CJ) a where n<=2

4、组内合并
select userId,collect_list(emb) emb_list from temp grop by userId

5、数组拆分多行
select pair[0] as pair_0,pair[1] as pair_1 from temp lateral view explode(seqs_pair(item_seq)) as pair

6、写入分区表
// 开启动态分区支持conf("hive.exec.dynamic.partition.mode", "nonstrict")
// SaveMode.Overwrite模式会覆盖所有分区
// SaveMode.Append模式,先清理当前分区再追加
// 分区验证
val partitions = spark.sql(s"show partitions ${db}.${table}")
partitions.show(false)
// 删除分区
partitions.collect().foreach(row =>
      if(row.getString(0).endsWith(dt)){
        logger.info(s"partition=$dt is exists, need drop")
        spark.sql(s"alter table ${db}.${table} drop partition(dt='$dt')")
      }
)
data.withColumn(partCol, lit(part)).write.mode(SaveMode.Append).insertInto(table)